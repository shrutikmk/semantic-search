{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "from odf import text, teletype\n",
    "from odf.opendocument import load\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(file_path):\n",
    "    \"\"\"Reads a file and returns its content as a string, supporting DOCX, PDF, ODT, and RTF files.\"\"\"\n",
    "    try:\n",
    "        if not os.access(file_path, os.R_OK):  # Checks if the file is readable\n",
    "            return \"The file is not readable.\"\n",
    "        \n",
    "        # Determine the file extension and process accordingly\n",
    "        _, file_extension = os.path.splitext(file_path)\n",
    "        \n",
    "        if file_extension.lower() == '.docx':\n",
    "            doc = Document(file_path)\n",
    "            return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        \n",
    "        elif file_extension.lower() == '.pdf':\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = ''\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + '\\n'\n",
    "                return text\n",
    "        \n",
    "        elif file_extension.lower() == '.odt':\n",
    "            odt_doc = load(file_path)\n",
    "            all_texts = teletype.extractText(odt_doc.text)\n",
    "            return all_texts\n",
    "        \n",
    "        elif file_extension.lower() == '.rtf':\n",
    "            with open(file_path, 'r') as file:\n",
    "                rtf_content = file.read()\n",
    "                return rtf_to_text(rtf_content)\n",
    "        \n",
    "        elif file_extension.lower() in ['.txt', '.csv', '.json']:  # Add more plaintext formats as needed\n",
    "            with open(file_path, 'r') as file:\n",
    "                return file.read()\n",
    "        \n",
    "        else:\n",
    "            return \"Unsupported file format.\"\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        return \"The specified file does not exist.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_chunk_text(text, chunk_size):\n",
    "    \"\"\"\n",
    "    Formats the given text to remove extra spaces and chunks it into parts of specified size.\n",
    "    \n",
    "    Parameters:\n",
    "    - text: The input string to format and chunk.\n",
    "    - chunk_size: The maximum size of each chunk.\n",
    "    \n",
    "    Returns:\n",
    "    A list of string chunks with each chunk being up to `chunk_size` characters long.\n",
    "    \"\"\"\n",
    "    # Format the text to replace multiple spaces with a single space and strip leading/trailing spaces\n",
    "    formatted_text = ' '.join(text.split())\n",
    "    \n",
    "    # Initialize variables for chunking\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for word in formatted_text.split():\n",
    "        # Check if adding the next word would exceed the chunk size\n",
    "        if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
    "            current_chunk += (word + \" \")\n",
    "        else:\n",
    "            # Append the current chunk to the list and start a new one\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = word + \" \"\n",
    "    \n",
    "    # Don't forget to add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents_in_folder(folder_path, chunk_size):\n",
    "    \"\"\"\n",
    "    Reads all documents in a specified folder, chunks them, and stores the chunks.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path: Path to the folder containing the documents.\n",
    "    - chunk_size: Size of each chunk.\n",
    "    \n",
    "    Returns:\n",
    "    A dictionary with document names as keys and lists of their chunks as values.\n",
    "    \"\"\"\n",
    "    documents_chunks = {}\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            content = read_file_content(file_path)\n",
    "            if isinstance(content, str):  # Ensure content was successfully read\n",
    "                chunks = format_and_chunk_text(content, chunk_size)\n",
    "                documents_chunks[file_name] = chunks\n",
    "    return documents_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_relevant_document(documents_chunks, query):\n",
    "    \"\"\"\n",
    "    Uses cosine similarity to find the most relevant document for a given query.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents_chunks: A dictionary with document names as keys and lists of chunks as values.\n",
    "    - query: The search query.\n",
    "    \n",
    "    Returns:\n",
    "    The name of the most relevant document.\n",
    "    \"\"\"\n",
    "    # Flatten the chunks for TF-IDF vectorization\n",
    "    doc_names = list(documents_chunks.keys())\n",
    "    all_texts = [' '.join(chunks) for chunks in documents_chunks.values()]\n",
    "    \n",
    "    # Add the query as the last item in the list for vectorization\n",
    "    all_texts.append(query)\n",
    "    \n",
    "    # Vectorize the text using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    \n",
    "    # Calculate cosine similarities between the query and all documents\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix[-1:], tfidf_matrix[:-1])\n",
    "    \n",
    "    # Find the index of the document with the highest cosine similarity\n",
    "    most_relevant_doc_index = np.argmax(cosine_similarities)\n",
    "    \n",
    "    # Return the name of the most relevant document\n",
    "    return doc_names[most_relevant_doc_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/Users/shrutikmk/Documents/compsci/Prakya/semantic-search/sample-docs'\n",
    "chunk_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_chunks = chunk_documents_in_folder(folder_path, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Knowledge is Power\"\n",
    "most_relevant_document = find_most_relevant_document(documents_chunks, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant document is: uChicago S1D3.txt\n"
     ]
    }
   ],
   "source": [
    "print(f\"The most relevant document is: {most_relevant_document}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
